{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis Project using Python\n",
    "\n",
    "\n",
    "EXploratory Data Analysis(EDA) is an important step in any data analysis it involves perform investigation on a given dataset to check anomalies, patterns and critical understanding of the dataset. \n",
    "\n",
    "It involves numerical summary and graphically presentation of the dataset. It helps a data scientist make sense of data before solving hypothesises and getting insights in a project.\n",
    "\n",
    "In this article i will try explain concepts and steps i undertook to perform simple EDA on Safety of Transport dataset collected to document security pain points and strategies adopted by different cities in the world to ensure inclusive development in public transit systems. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with I imported all the necessary libraries for the assignemnt. Some of the libraries thati have used are Pandas, numpy, matplotlib, seaborn.\n",
    "\n",
    "Panda is a library for manipulation of data. It is an open source library which provides high performance, easy to use data structure for Python Programming.\n",
    "\n",
    "Numpy is a library that comes in when working with arrays. It provides multidimentional matrices and arrays for large data set. It comes with many mathematical functions to work on these arrays. \n",
    "\n",
    "Matplotlib is a library that assists in graphical and visualization work. Matplotlib and numpy work hand in hand. Together they create an alternative to Matlab.\n",
    "\n",
    "Seaborn is also an alternative library for data visualization and is based on matplotlib.\n",
    "\n",
    "OS library is used for interacting with your machines operating system. It maps scripts to the actual environments that you are working on incase you change location of the project in your computer drive. This module is optional and I just have it there for my own. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "pwd=os.getcwd()\n",
    "%matplotlib inline \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an outline of what we are going to cover on our script. \n",
    "* Getting/Importing the data\n",
    "* Data Preparation and Cleaning\n",
    "* Exploratory analysis and Visualizations.\n",
    "* Summary  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data & Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we us the Pandas library to import the required dataset into python. You can read many formats of data using the Pandas library (CSV,html,Json etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv ('G:/Transport Safety Project/Public_Transit_Survey.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the data and how it looks like we use the .head() command to check the first five rows of the dataset and .tail() to check the last five rows of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also to see the whole dataset you need to recall the name given to the dataframe in this case we recall (data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to check the data types of each column. The dtype returns a series with data type of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the same thing as the dtype code. The only difference is the df.info() command prints additional infomation (non-null values, memory usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the variables/Columns names on our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we get names of data frames. Its important for later on when doing some certain operation especially when having a huge dataset.\n",
    "There are several ways of doing this:\n",
    "* Sorted() method\n",
    "* tolist() method\n",
    "* column.values method\n",
    "* keys() function\n",
    "* columns attritube with dataframe object\n",
    "* Iterating over columns\n",
    "\n",
    "For this project we shall use this two (iterating over columns method) and the (columns attribute with dataframe object) as examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns attribute with dataframe object\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterating over columns method\n",
    "for col in data.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will always get missing data when no information is provided on one or more item in a unit. We get missing data because either it never existed or it was not collected.\n",
    "In Pandas mising data is represented by two values \n",
    "* NaN\n",
    "* None\n",
    "Pandas uses the two NaN and None interchangebly to indicate null or missing values.\n",
    "\n",
    "\n",
    "\n",
    "There are multiple function for detecting, replacing and removing null values in a dataframe:\n",
    "* notnull()\n",
    "* dropna()\n",
    "* replace()\n",
    "* isnull()\n",
    "* interpolate()\n",
    "* fillna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().tail() # .isnull() checks for null values. .tail() brings the last five rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".isnull () method checks and manage Null values in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sum up the number of missing values in all rows by adding the .sum() function after the .isnull() after the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Droping Columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing EDA it is important to drop unneccesary columns those that will not contribute much on our analysis. To know which column to drop you need to understand the data very well. So before droping and analysis make sure you have a clear understanding of the data.\n",
    "\n",
    "To drop column we us the .drop() functon which remove columns and rows by specifying the exact names and corresponding axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data.drop(['group1-note', 'group1-note2', 'location-Accuracy', 'meta-instanceID', 'KEY', 'SubmitterID','SubmitterName', 'AttachmentsPresent', 'AttachmentsExpected', 'Status','ReviewState', 'DeviceID', 'Edits'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning/dealing with NA data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next process is dealing with the NA values. The NA values are those values that are missing. We have to know what to do with missing values. Missing values interferes with our analysis. Missing values can either be removed or replaced depending on the data. It is important to ask about missing values from Data owners before you do anything about the missing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fillna() function is used to replaces missing values in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    data[column].fillna(data[column].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.fillna(method = 'bfill', axis=0).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change value names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of data preparation is editing some values in a table that seemed incorectly spelt or written. In my data in the column frequency oftenly had been misspelt. For the data to give us a clear value we needed to do something about that. \n",
    "\n",
    "loc() is a function that can be used to update value of a row with respect to the column by providing the labels of the columns and the index of the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['frequency']== \"only\", \"frequency\"]= \"oftenly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts/Groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggest the groupby() function involves several operations at ones; splitting the odject, applying a function, and combinng the results. \n",
    "\n",
    "It returns a groupby object which contains information about the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count= data['gender'].value_counts()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count= data['frequency'].value_counts(normalize=True)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Double_Counts = data.groupby('gender')['frequency'].value_counts(normalize= False)\n",
    "Double_Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Double_Counts = data.groupby('forms')['harassment'].value_counts(normalize= True)\n",
    "Double_Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Double_Counts = data.groupby('gender')['gender_safety'].value_counts(normalize= True)\n",
    "Double_Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Dataframe to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last code is used to convert the Data to CSv. When you want to take a clean data to other tools for maybe visuaization purpose you can convert the dataframe to CSV format using .to_CSV() function. I have saved it in Drive G on a folder named Transport safety Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv( r'G:\\Transport Safety Project\\Clean Data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_excel(pwd +\"final_output.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
